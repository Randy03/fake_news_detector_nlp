{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e863d044-1c23-4ad5-9d07-b60dc58781ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e211a792-3775-4afb-95b0-41788d814ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('fake_or_real_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be0b62ac-24a2-48b8-8c83-71ee60a2bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Custom transformer que limpia el texto de caracteres especiales, quita stopwords y aplica lemmatizer\n",
    "    '''\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.str.lower().str.strip()\n",
    "        translator = str.maketrans('', '', string.punctuation+'’‘—“”–')\n",
    "        X = X.map(lambda x: x.strip().lower().translate(translator))\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        X = X.map(word_tokenize).apply(lambda x: [word for word in x if word not in stopwords])\n",
    "        X = X.apply(lambda x: [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in x])\n",
    "        X = X.apply(lambda x: ' '.join(x)).to_numpy()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "459ed412-268a-42e7-9831-ceef2a5abf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"text\", TextTransformer()),\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", PassiveAggressiveClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "247b91ea-11f0-4340-b022-f964f549e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['label'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d99edbf-ee97-4d8e-8468-49542047cb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('text', TextTransformer()), ('tfidf', TfidfVectorizer()),\n",
       "                ('clf', PassiveAggressiveClassifier())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe7940c6-0e35-49be-8087-26e9b3ac7882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9201014584654407"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f76fa62c-e65a-46e7-8437-8bc45c9ba9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('text', TextTransformer()),\n",
       "                                             ('tfidf', TfidfVectorizer()),\n",
       "                                             ('clf',\n",
       "                                              PassiveAggressiveClassifier())]),\n",
       "                   n_iter=25, n_jobs=-1,\n",
       "                   param_distributions={'clf__C': (1.0, 1.5, 2.0),\n",
       "                                        'clf__early_stopping': (True,),\n",
       "                                        'clf__max_iter': (500, 1000, 2000),\n",
       "                                        'tfidf__max_df': (0.5, 0.75, 1.0),\n",
       "                                        'tfidf__max_features': (None, 5000,\n",
       "                                                                10000, 50000),\n",
       "                                        'tfidf__ngram_range': ((1, 1), (1, 2)),\n",
       "                                        'tfidf__norm': ('l1', 'l2'),\n",
       "                                        'tfidf__use_idf': (True, False)},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"tfidf__max_df\": (0.5, 0.75, 1.0),\n",
    "    \"tfidf__max_features\": (None, 5000, 10000, 50000),\n",
    "    \"tfidf__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    \"tfidf__use_idf\": (True, False),\n",
    "    \"tfidf__norm\": ('l1', 'l2'),\n",
    "    \"clf__C\": (1.0,1.5,2.0,),\n",
    "    \"clf__max_iter\": (500,1000, 2000),\n",
    "    \"clf__early_stopping\": (True,False)\n",
    "}\n",
    "\n",
    "cv = RandomizedSearchCV(pipeline, parameters, n_jobs=-1, verbose=1,n_iter=25)\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84710c4-0da9-4a3c-a84e-8506f1e9f17c",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96ab4f59-ad72-4c09-af72-0e6a241e2b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9283449587824985"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7b9a4d0-dd6d-4c8e-9bc1-9085965223f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('text', TextTransformer()),\n",
       "  ('tfidf', TfidfVectorizer(max_features=50000, ngram_range=(1, 2))),\n",
       "  ('clf',\n",
       "   PassiveAggressiveClassifier(C=2.0, early_stopping=True, max_iter=500))],\n",
       " 'verbose': False,\n",
       " 'text': TextTransformer(),\n",
       " 'tfidf': TfidfVectorizer(max_features=50000, ngram_range=(1, 2)),\n",
       " 'clf': PassiveAggressiveClassifier(C=2.0, early_stopping=True, max_iter=500),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': 50000,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'clf__C': 2.0,\n",
       " 'clf__average': False,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__early_stopping': True,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__loss': 'hinge',\n",
       " 'clf__max_iter': 500,\n",
       " 'clf__n_iter_no_change': 5,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__random_state': None,\n",
       " 'clf__shuffle': True,\n",
       " 'clf__tol': 0.001,\n",
       " 'clf__validation_fraction': 0.1,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0b5d1fc-37e9-450f-977b-7e66f7c972c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv.best_estimator_, open('pipeline', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b28173-7ac7-4963-9ab9-2cafa07853c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
